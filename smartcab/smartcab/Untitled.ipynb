{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "Couldn't open images/car-orange.png",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2189bb72adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-f2189bb72adc>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Now simulate it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reduce update_delay to speed up simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m108\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# press Esc or close pygame window to quit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m#print 'Success rate: ',(100.0-a.failure_times)/100.0,'  penalty_times: ',a.penalty_times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jayden/Desktop/smartcab/smartcab/simulator.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, size, frame_delay, update_delay)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_circle_radius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# radius of circle, when using simple representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sprite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"car-{}.png\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_sprite_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sprite_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sprite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sprite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Couldn't open images/car-orange.png"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "from collections import OrderedDict\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.alpha=1 # learning rate\n",
    "        self.timer=1 # timer for modify learning rate\n",
    "        self.lamda=0 # reward discount parameter to adjust\n",
    "        self.life=0;\n",
    "        self.failure_times=0;\n",
    "        self.q_value=OrderedDict(); # Q value\n",
    "        self.q_value_ground_truth=OrderedDict();# Q value ground true use as a metric\n",
    "        self.q_inti_value_Flag=True; # whether to initialize Q value\n",
    "        self.penalty_times=0\n",
    "        self.learn_time=8\n",
    "        self.q_value_valid=[]\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        self.timer=1\n",
    "        self.life=self.life+1\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "        if deadline == 0 and self.life>=self.learn_time:\n",
    "            self.failure_times+=1\n",
    "        # TODO: Update state\n",
    "        #Available state\n",
    "        AvailableInformation=[('Next Way Point:', self.next_waypoint),\n",
    "        ('light',inputs['light']),\n",
    "        ('On Comming:',inputs['oncoming']),\n",
    "        ('Left:',inputs['left']),\n",
    "        ('Right:',inputs['right']),\n",
    "        ('Deadline',deadline)]\n",
    "        # validComingAgents=[None, 'right', 'left', 'leftRight','forward',\n",
    "        # 'forwardRight','forwardLeft','forwardLeftRight']\n",
    "        # rightCar = 1 if \n",
    "        # comingAgents=validComingAgents[]\n",
    "\n",
    "        self.state=AvailableInformation[0:2];# states without deadline,just light and next_waypoint as state\n",
    "        # print 'Model>>>>  Light: ',inputs['light'],\"  Next_waypoint:\", self.next_waypoint\n",
    "        # TODO: Select action according to your policy\n",
    "        # random action\n",
    "\n",
    "        action= random.choice(self.env.valid_actions)\n",
    "        if self.q_inti_value_Flag==False and self.life>=self.learn_time:\n",
    "            # print 'QValue>>>> Forward: ',self.q_value[(inputs['light'],self.next_waypoint,'forward')],\\\n",
    "            #     'Left: ',self.q_value[(inputs['light'],self.next_waypoint,'left')],\\\n",
    "            #     'Right: ',self.q_value[(inputs['light'],self.next_waypoint,'right')]\n",
    "            ran=random.randint(1,10)#x% probability to follow Q\n",
    "            if ran>3: # parameter to adjust\n",
    "                action = self.get_max_a_r(inputs['light'],self.next_waypoint)[1]\n",
    "        # Execute action and get reward\n",
    "        # print 'Action>>>> ', action\n",
    "        # print '-------------------------------------------'\n",
    "        reward = self.env.act(self, action)\n",
    "        if reward<0 and self.life>=self.learn_time:\n",
    "            self.penalty_times=self.penalty_times+1\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "       \n",
    "        #initial Q value only do once on the robot born set all value to zero\n",
    "        if self.q_inti_value_Flag:\n",
    "            for light_cond in ['green', 'red']:\n",
    "                for next_waypoint_cond in ['forward','left','right']:\n",
    "                    for act in self.env.valid_actions:\n",
    "                        self.q_value[(light_cond,next_waypoint_cond,act)]=0\n",
    "                        \n",
    "                        # initial ground truth\n",
    "                        if act == None:\n",
    "                            self.q_value_ground_truth[(light_cond,next_waypoint_cond,act)]=1;\n",
    "                        else:\n",
    "                            if light_cond == 'red' and act != 'right':\n",
    "                                self.q_value_ground_truth[(light_cond,next_waypoint_cond,act)]=-1;\n",
    "                            else:\n",
    "                                self.q_value_ground_truth[(light_cond,next_waypoint_cond,act)]= 2 if act == next_waypoint_cond else 0.5 \n",
    "\n",
    "\n",
    "                        self.q_inti_value_Flag=False\n",
    "        #update Q value by equation q=(1-alpha)*q+alpha(reward+lamda*expected_next_reward)\n",
    "        else:\n",
    "            # get new state after the action to calculate expected_next_reward\n",
    "            self.alpha=1/float(self.timer)\n",
    "            new_next_waypoint = self.planner.next_waypoint() # \n",
    "            new_inputs = self.env.sense(self)\n",
    "\n",
    "            self.q_value[(inputs['light'],self.next_waypoint,action)]=\\\n",
    "                (1-self.alpha)*self.q_value[(inputs['light'],self.next_waypoint,action)]+\\\n",
    "                self.alpha*(reward+self.lamda*self.get_max_a_r(new_inputs['light'],new_next_waypoint)[0])\n",
    "        if self.life<self.learn_time:\n",
    "            self.q_value_valid.append(self.get_q_valid())\n",
    "        #print \"LearningAgent.update(): deadline = {}, inputs = {}, action = {}, reward = {}\".format(deadline, inputs, action, reward)  # [debug]\n",
    "    # define a function to get Q and action\n",
    "    # input light_cond: current light condiction\n",
    "    # input next_waypoint_cond: current next waypoint\n",
    "    # output (expected Q value, best action)\n",
    "    def get_max_a_r(self,light_cond,next_waypoint_cond):\n",
    "        acts=['forward','left','right',None]\n",
    "        values = (self.q_value[light_cond,next_waypoint_cond,'forward'],\n",
    "            self.q_value[light_cond,next_waypoint_cond,'left'],\n",
    "            self.q_value[light_cond,next_waypoint_cond,'right'],\n",
    "            self.q_value[light_cond,next_waypoint_cond,None])\n",
    "        maxValue =max(values)\n",
    "        act_choices_index=[]\n",
    "\n",
    "        for index in range(0,4):\n",
    "            if values[index]==maxValue:\n",
    "                act_choices_index.append(index)\n",
    "        act=random.choice([acts[i] for i in act_choices_index])\n",
    "        return (maxValue,act)\n",
    "\n",
    "\n",
    "    def get_q_valid(self):\n",
    "        q_vector=[]\n",
    "        q_ground_truth=[]\n",
    "        for light_cond in ['green', 'red']:\n",
    "                for next_waypoint_cond in ['forward','left','right']:\n",
    "                    for act in self.env.valid_actions:\n",
    "                        q_vector.append(self.q_value[light_cond,next_waypoint_cond,act])\n",
    "                        q_ground_truth.append(self.q_value_ground_truth[light_cond,next_waypoint_cond,act])\n",
    "        q_vector=np.array(q_vector)\n",
    "        q_ground_truth=np.array(q_ground_truth)\n",
    "        q_vector_l2_norm=np.sum(q_vector*q_vector)**0.5\n",
    "        q_ground_truth_l2_norm=np.sum(q_ground_truth*q_ground_truth)**0.5\n",
    "        valid=np.sum(q_vector*q_ground_truth)/(q_vector_l2_norm*q_ground_truth_l2_norm)\n",
    "        return valid\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "\n",
    "    # Set up environment and agent\n",
    "    e = Environment()  # create environment (also adds some dummy traffic)\n",
    "    a = e.create_agent(LearningAgent)  # create agent\n",
    "    e.set_primary_agent(a, enforce_deadline=True)  # set agent to track\n",
    "\n",
    "    # Now simulate it\n",
    "    sim = Simulator(e, update_delay=0.01)  # reduce update_delay to speed up simulation\n",
    "    sim.run(n_trials=108)  # press Esc or close pygame window to quit\n",
    "    #print 'Success rate: ',(100.0-a.failure_times)/100.0,'  penalty_times: ',a.penalty_times\n",
    "    \n",
    "    print 'Q Value: ',a.q_value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
